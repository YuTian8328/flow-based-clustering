def algorithm_sbm(graph,sampleset,lambda_nlasso=0.1,K=20,penalty_norm="1norm"):
    '''
    Algorithm simulating distributed environment
    Input parameters
        graph: a already setup graph with nodes atrributes and edge weights
        sampleset: indicators of labeld nodes
        lambda_nlasso: regularizer hyperparameter for GTV term
        K: number of iterations
        penalty_norm: the choice for GTV term, "1norm","2norm" or "sq2norm"
    Returns
        currweights: learned weights for each nodes
        iter_mse: a list of mse calculated for each iteration
    '''
    iter_mse = []
    num_nodes = len(g.nodes)
    for iter_algo in range(K):
        oldnodeweight=nx.get_node_attributes(g,"weight")  # read in current node weight vectors
        # iterate nodes to update nodes associated parameters
        for nodevar in g.nodes(data=False):
            oldweight = oldnodeweight[nodevar]
            dmy=np.zeros(oldweight.shape)
            # iterate over all neighbors of node to realize the update step of Algorihm 1 lines 2-4
            for node_j in g[nodevar]: 
                edgeweighttmp =  g.edges[nodevar,node_j]["weight"]
                if node_j > nodevar:
                    dmy = dmy-edgeweighttmp
                else: 
                    dmy = dmy+edgeweighttmp
            dmy= dmy/g.degree(nodevar); 
            regularizerterm = oldweight - dmy 
            # primal update step of Algorithm 1 lines 5-7
            if nodevar in sampleset:
                optimizer = g.nodes[nodevar]["optimizer"]
                model = g.nodes[nodevar]["model"]
                model.linear.weight.data=torch.from_numpy(np.array(regularizerterm, dtype=np.float32)) 
                y_data = g.nodes[nodevar]["label"]
                x_data = g.nodes[nodevar]["features"]
                criterion = g.nodes[nodevar]["criterion"]
                taufac = g.nodes[nodevar]["degree"]
                # Zero gradients, perform a backward pass, and update the weights.
                for iterinner in range(5):
                    def closure():
                        optimizer.zero_grad()
                        y_pred = model(x_data) 
                        loss1 = criterion(y_pred, y_data)
                        loss= loss1+0.5*taufac*torch.mean((model.linear.weight-torch.from_numpy(regularizerterm))**2) 
                        loss.backward()
                        return loss
                    optimizer.step(closure)
                g.nodes[nodevar]["weight"] = model.linear.weight.data.numpy()
            else:
                g.nodes[nodevar]["weight"] = regularizerterm
        # iterate over edges to realize the dual update step of Algorithm 1 lines 8-11
        for edgevar in g.edges(data=False): 
            tmpweight =  g.edges[edgevar]["weight"]
            tailnode=np.min([edgevar[0],edgevar[1]])
            headnode=np.max([edgevar[0],edgevar[1]])
            tmpnodew1 = 2*g.nodes[headnode]["weight"]-oldnodeweight[headnode]
            tmpnodew2 = 2*g.nodes[tailnode]["weight"]-oldnodeweight[tailnode]
            tmp =  tmpweight+(1/2.1)*(tmpnodew1-tmpnodew2)
            if penalty_norm == "1norm":
                g.edges[edgevar]["weight"] = np.clip(tmp,-lambda_nlasso,lambda_nlasso)
            elif penalty_norm  == "2norm":
                g.edges[edgevar]["weight"] = tmp/np.max([1,np.linalg.norm(tmp,2)/lambda_nlasso])
            elif penalty_norm == "sq2norm":  
                g.edges[edgevar]["weight"] = tmp*lambda_nlasso/(lambda_nlasso+1/2.1)
            else:
                raise ValueError(f"Invalid Norm")
        temp_weights = nx.get_node_attributes(g,"weight")
        currweights = []
        for nodeidx in g.nodes(data=False):
            currweights.append(temp_weights[nodeidx][0])
        currweights = np.array(currweights)
                
        iter_mse.append(get_MSE(g,currweights,num_nodes))
    return currweights,iter_mse